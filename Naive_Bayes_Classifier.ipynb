{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with Naive Bayes\n",
    "\n",
    "### Create benchmarking modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(266, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The results in 2nd line treatment show an ORR ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The long duration of response and high durable...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence  Positive  Negative  \\\n",
       "ID                                                                          \n",
       "1   The results in 2nd line treatment show an ORR ...         1         0   \n",
       "2   The long duration of response and high durable...         1         0   \n",
       "\n",
       "    Neutral  \n",
       "ID           \n",
       "1         0  \n",
       "2         0  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "  \n",
    "df = []\n",
    "df = pd.read_excel('sentences_with_sentiment.xlsx', sheet_name = 'Sheet1')\n",
    "\n",
    "df.set_index('ID', inplace=True)\n",
    "\n",
    "print(df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The results in 2nd line treatment show an ORR ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The long duration of response and high durable...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The median OS time in the updated results exce...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Therefore, the clinical benefit in 2nd line tr...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The data provided in 1st line, although prelim...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence    labels\n",
       "ID                                                             \n",
       "1   The results in 2nd line treatment show an ORR ...  Positive\n",
       "2   The long duration of response and high durable...  Positive\n",
       "3   The median OS time in the updated results exce...   Neutral\n",
       "4   Therefore, the clinical benefit in 2nd line tr...  Positive\n",
       "5   The data provided in 1st line, although prelim...  Positive"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = []\n",
    "\n",
    "for i in df.index:\n",
    "    cont = 0\n",
    "    if df.loc[i,'Positive'] == 1:\n",
    "        labels.append('Positive')\n",
    "        cont +=1\n",
    "        \n",
    "    if df.loc[i,'Negative'] == 1:\n",
    "        labels.append('Negative')\n",
    "        cont +=1\n",
    "        \n",
    "    if df.loc[i,'Neutral'] == 1:\n",
    "        labels.append('Neutral')\n",
    "        cont+=1\n",
    "        \n",
    "    if cont != 1:\n",
    "        raise(\"Fehler Label\")\n",
    "        \n",
    "df['labels'] = labels\n",
    "df.drop(['Positive','Negative','Neutral'],axis=1,inplace=True)\n",
    "\n",
    "### Shuffle Labels to test Classifier\n",
    "#df['labels'] = np.random.permutation(df['labels'].values)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((226,), (40,), ID\n",
       " 16     Positive\n",
       " 194     Neutral\n",
       " Name: labels, dtype: object, ID\n",
       " 182    Positive\n",
       " 120     Neutral\n",
       " Name: labels, dtype: object)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.Sentence, df.labels, test_size = 0.15, random_state = 42)\n",
    "X_train.shape, X_test.shape, y_train.head(2), y_test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized Representation - A lot can be done here, lemmantize, n-grams, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(226, 1100)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words = 'english')\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# transform a count matrix to a normalized tf-idf representation (tf-idf transformer)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Classifier\n",
    "#### Compare \"normal\" Multinomial Bayes vs. Complement Bayes (works better with umbalanced data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "clf = ComplementNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_counts = vectorizer.transform(X_test)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "predicted = clf.predict(X_test_tfidf)\n",
    "probabilities = clf.predict_proba(X_test_tfidf) ### Get the Probabilities of each sample belonging to a class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Classifier\n",
    "#### Cross-Validation can also me implemented for evaluation\n",
    "\n",
    "#### In problems with strong class imbalance, a model can be missleasing, achievieng high accuracy if it predicts the value of the majority class for all predictions.\n",
    "\n",
    "#### Precision is the number of True Positives divided by the number of True Positives and False Positives. A low precision can also indicate a large number of False Positives.\n",
    "\n",
    "#### Recall is the number of True Positives divided by the number of True Positives and the number of False Negatives.Recall can be thought of as a measure of a classifiers completeness. A low recall indicates many False Negatives.\n",
    "\n",
    "#### F1 score conveys the balance between the precision and the recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy : 96.90265486725663 %\n",
      "\n",
      "Test Set Accuracy : 65.0 % \n",
      "\n",
      "\n",
      "Classifier Report : \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.67      0.57      0.62         7\n",
      "     Neutral       0.50      0.27      0.35        11\n",
      "    Positive       0.68      0.86      0.76        22\n",
      "\n",
      "   micro avg       0.65      0.65      0.65        40\n",
      "   macro avg       0.62      0.57      0.58        40\n",
      "weighted avg       0.63      0.65      0.62        40\n",
      "\n",
      "Confussion Matrix: \n",
      " [[ 4  0  3]\n",
      " [ 2  3  6]\n",
      " [ 0  3 19]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the classifier\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, predicted)\n",
    "prediction = clf.predict(X_test_tfidf)\n",
    "prediction_train = clf.predict(X_train_tfidf)\n",
    "  \n",
    "print(f\"Training Set Accuracy : {accuracy_score(y_train, prediction_train) * 100} %\\n\")\n",
    "print(f\"Test Set Accuracy : {accuracy_score(y_test, prediction) * 100} % \\n\\n\")\n",
    "print(f\"Classifier Report : \\n\\n {classification_report(y_test, prediction)}\")\n",
    "print(f\"Confussion Matrix: \\n {confusion_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.21049272, 0.20269852, 0.58680876]), 'Positive')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 4\n",
    "probabilities[i], prediction[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Negative', 'Neutral', 'Positive'], dtype='<U8')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
